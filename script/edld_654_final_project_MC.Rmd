---
title: "edld_654_final_project_MC"
author: "Michelle Cui"
date: "2025-11-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

-   **Research problem (10pts)**: Describe the task you want to achieve. What is the outcome of interest? What are you trying to predict? Why is it important? What are the potential benefits of having a predictive model for this outcome?

-   **Description of the data (15pts)**: Describe core features of the data, any additional features you produced from existing features and how, basic descriptive statistics about these features, and any missing data analysis you conduct. The description should be sufficiently clear that the instructor understands all the variables included in your modeling.

The main objective of this task is to explore several supervised learning models for predicting the outcome of interest. The Dataset I used is the US teacher questionnaire survey data provided in the International Computer and Information Literacy Study(ICILS) 2023.

Which teacher-level factors predict how frequently teachers have 8-th grade students write computer programs, games, or apps during classroom instruction?
Whether teacher-level factors can predict how frequently teachers have 8-th grade students write computer programs, games, or apps during classroom instruction?

The ICILS is designed to measure 8th-grade students' capabilities to use Computer Information Literacy(CIL) and Computational Thinking(CT). Along with 35 education systems, United States participated CIL assessment and CT assessment with 24 education systems. The study survey included context questionnaire for school principles & coordinators, teachers, and 8th-grade students. It is to my interest that, the study didn't include the U.S. teacher data in the international average or presented in the main ICILS 2023 international report because the response rate for sampled teachers in the United States was below 50 percent. The correlated factors could be varied. Thus, I plan to use this dataset to explore more about teachers' usage of ICT in the U.S.

-   **Description of the models (15pts)**: Apply at least three different modeling approaches to predict the outcome in the dataset. Describe any specific setting used during the model fitting (e.g., hyperparameter tuning, cross-validation). Also, discuss how you plan to evaluate model performance.

-   **Model fit (20pts)**: Provide the results of your model evaluation. Compare and contrasts results from different modeling approaches, including a discussion of model performance. Discuss your final model selection and the evidence that led you to this selection. If it is a classification problem, how did you choose a cut-off point for binary predictions? Did you consider different cut-off points?

-   **Data visualization (5pts)**: Include at least two plots (or more) to help communicate your findings. The plots may be of initial data explorations, fits of individual models, and plots displaying the performance of competing models.

-   **Discussion/Conclusion (25pts)**: Discuss and summarize what you learned. Which variables were the most important in predicting your outcome? Was this expected or surprising? Were different models close in performance, or were there significant gaps in performance from different modeling approaches? Are there practical/applied findings that could help the field of your interest based on your work? If yes, what are they?

Should I use multinomial regression?

-   **Reproducibility (10pts)**: Provide a link to the GitHub repo at the beginning of your report as a note.

```{r}
require(here)
require(tidyverse)
require(recipes)
require(ggplot2)
require(labelled)
require(stringr)
require(haven) #zap_labels function
```
## 1. Initial Data Preparation
```{r}
load(here("data","BTGUSAI3.Rdata"))
teacher <- BTGUSAI3
names(teacher)
```

```{r}
vars_to_remove <-grep(
  pattern = "^(TRWGT|WGT|JKZONET|JKREPT|SENWGTT|VERSION|HOUWGTT|TOTWGTT|T_)",
                       names(teacher), value = TRUE)
vars_to_remove
teacher_clean <- teacher[, !names(teacher) %in% vars_to_remove]
```

```{r}
#remove SPSS labels without change numeric and label
teacher_clean <- 
  teacher_clean %>% 
  zap_labels() %>% 
  zap_label()
```

```{r}

teacher_clean$IT3G17J_f <- factor(
  teacher_clean$IT3G17J,
  levels = c(1,2,3,4,5),
  labels = c(
    "Never",
    "≤ 1/4",
    "≤ 1/2",
    "1/2–3/4",
    "≥ 3/4"
  )
)

ggplot(teacher_clean, aes(IT3G17J_f, fill = IT3G17J_f)) +
  geom_bar() +
  geom_text(stat = "count",
            aes(label = after_stat(count)),
            vjust = -0.5,
            size = 4)+
  theme_minimal() +
  labs(title = "Distribution of Programming Assignment Frequency",
       x = "Write computer programs, games, or apps",
       y = "Count")
```

-   I collapsed the outcome variable into a binary indicator that reflecting whether the teacher ever engages students in computer programming activities. The original outcome is highly skewed toward "Never", and the categories above "Never" are sparse. It would be risky for me to use ordinal modeling. 1: Never; 2: One quarter or less (but more than never); 3: One half or less (but more than one quarter); 4: More than one-half (but less than three quarters); 5: Three quarters or more

```{r}
teacher_clean <- teacher_clean %>% 
  mutate(
    assign_pro_bi = ifelse(IT3G17J == 1,1,0)
  ) %>% 
  select(-IT3G17J,-IT3G17J_f,-IDCNTRY,-CNTRY, -IEADATE,-IDLANG_TCQ)
```

```{r}
# Initial data preparation
require(finalfit)
ff_glimpse(teacher_clean)

require(dplyr)

# identify variables
categorical_vars <- teacher_clean %>% 
  summarise(across(everything(), ~n_distinct(.x, na.rm = TRUE))) %>% 
  pivot_longer(everything(), names_to = "variable", values_to = "unique_n") %>% 
  filter(unique_n <= 10 | 
           sapply(teacher_clean, is.character)[match(variable, names(teacher_clean))]) %>% 
  pull(variable)

categorical_vars


# Move IDSCHOOL to categorical_var
teacher_clean$IDSCHOOL <- as.factor(teacher_clean$IDSCHOOL)
categorical_vars <- c(categorical_vars, "IDSCHOOL") %>% 
  unique()

continuous_vars <- setdiff(names(teacher_clean), categorical_vars)

continuous_vars

```

```{r}
# Convert all categorical variables to factors
teacher_clean <- teacher_clean %>% 
  mutate(across(all_of(categorical_vars), as.factor))
```

Missing summary table

```{r}
missing_ <- ff_glimpse(teacher_clean)$Continuous
head(arrange(missing_, desc(missing_percent)))

```

### 1.1 Group variables into different types

```{r}

outcome <- c('assign_pro_bi')

id      <- c("IDTEACH")

categorical <- setdiff(categorical_vars, outcome)

teacher_clean <- teacher_clean %>% 
  mutate(across(all_of(categorical), as.factor))

teacher_clean$assign_pro_bi <- factor(teacher_clean$assign_pro_bi,
                                      levels = c(0,1),
                                      labels = c("AnyUse","Never")) 

continuous <- setdiff(continuous_vars,id)

```

Build blueprint recipes to prepare data: I implemented the following steps:

-   Creating missing data variable indicators for: all_of(categorical) all_of(continuous)

-   Zero variance filter on: all_numeric()

-   Mean imputation for: all_of(continuous)

-   Mode imputation for: all_of(categorical)

-   Polynomial expansion on: all_of(numeric)

-   Centering and scaling for: paste0(continuous, "\_poly_1") paste0(continuous, "\_poly_2")

-   Dummy variables from: all_of(categorical)

-   Factor variables from: assign_pro_bi

### 1.2 Prepare a recipe

```{r}

require(recipes)
options(warn=-1)

blueprint <- recipe(assign_pro_bi ~ ., data =  teacher_clean) %>% 
  #update_role(assign_pro_bi, new_role = "outcome") %>% 
  update_role(IDTEACH, new_role = "ID") %>% 
  
  # For all 203 predictors, create an indicator variable for missingness
  
  step_indicate_na(all_predictors()) %>% 
  
  # Remove the variable with zero variance, this will also remove 
  # The missingness variables
  
  step_zv(all_predictors()) %>% 
  
  # impute the missing values using mean and mode
  
  step_impute_mean(all_of(continuous)) %>% 
  step_impute_mode(all_of(categorical)) %>% 
  
  # 2nd degree polynomial terms for continuous variables
  
  step_poly(all_of(continuous),degree = 2) %>% 
  
  # Standardize the polynomial terms of continuous variables
  
  step_normalize(all_numeric_predictors()) %>% 
  
  # One-hot encoding for all categorical variables
  
  step_dummy(all_nominal_predictors(), one_hot = TRUE)
 
 #step_num2factor(assign_pro_bi,
                  #transform = function(x)x + 1,
                  #levels = c("Any use","Never"))

blueprint
```

### 1.3 Train/Test data split

```{r}
require(caret)

teacher_clean <- teacher_clean %>% 
  filter(!is.na(assign_pro_bi))

loc <- sample(1:nrow(teacher_clean), round(nrow(teacher_clean)*0.8))

teacher_tr <- teacher_clean[loc, ]
teacher_te <- teacher_clean[-loc, ]

dim(teacher_tr)
dim(teacher_te)
```

## 2. Building a Logistic Regression Model with No Regularization


```{r}
# Randomly shuffle the training dataset
set.seed(11252025)
teacher_tr = teacher_tr[sample(nrow(teacher_tr)), ]

# Create 10 folds with equal size
folds = cut(seq(1,nrow(teacher_tr)), breaks = 10, labels = FALSE)

# Create the list for each fold
my.indices <- vector('list', 10)

for (i in 1:10){
  my.indices[[i]] <- which(folds != i)
}
```

```{r}
cv <- trainControl(
  method    = "cv",
  index     = my.indices,
  classProbs = TRUE,
  summaryFunction = mnLogLoss,
  savePredictions = "final" # confirm positive class
)
```

### 2.1 Train the logistic regression model

```{r}

caret <- train(blueprint,
                   data   = teacher_tr,
                   method = "glm",
                   family = "binomial",
                   metric = "logLoss",
                   trControl = cv)

caret
```

```{r}
LL_mod <- caret$results$logLoss
```

### 2.2 Logistic Regression Performance Evaluation

```{r}
predicted_te <- predict(caret, teacher_te, type = 'prob')
dim(predicted_te)
```

```{r}
head(predicted_te)
```


```{r}
group_0 <- which(teacher_te$assign_pro_bi=="AnyUse")
group_1 <- which(teacher_te$assign_pro_bi=="Never")

prob_0 <- jitter(predicted_te$Never[group_0], amount=1e-4)
prob_1 <- jitter(predicted_te$Never[group_1], amount=1e-4)

plot(density(prob_0, adjust=2), col = "lightblue", xlab='Predicted Probability for Never', main = "")
lines(density(prob_1, adjust=2),col = "orange",lty=2)
legend("topright", c("AnyUse","Never"), col = c("lightblue","orange"), lty=c(1,2), bty='n')


```


```{r}
require(cutpointr)

cut.obj <- cutpointr(x = predicted_te$Never,
                     class = teacher_te$assign_pro_bi)

cut.obj

AUC <- auc(cut.obj)
```

```{r}
# Confusion matrix assuming the threshold is 0.5

pred_class <- ifelse(predicted_te$Never>.5,1,0)

confusion <- table(teacher_te$assign_pro_bi,pred_class)

confusion
```


```{r}
# True Negative Rate
TNR <- confusion[1,1]/(confusion[1,1]+confusion[1,2])

# False Positive Rate
FPR <- confusion[1,2]/(confusion[1,1]+confusion[1,2])

# True Positive Rate
TPR <- confusion[2,2]/(confusion[2,1]+confusion[2,2])

# Precision
PRE <- confusion[2,2]/(confusion[1,2]+confusion[2,2])

# overall acuracy
ACC <- (confusion[2,2]+confusion[1,1])/(confusion[2,2]+confusion[1,1]+confusion[1,2]+confusion[2,1])
```


## 3. Building a Classification Model with Ridge Penalty

```{r}
grid_ridge <- data.frame(alpha = 0, lambda = c(seq(0,.001,.00001),.005,.01,.05,.1)) # bench mark values
head(grid_ridge)

```

### 3.1 Train the logistic regression model

```{r}
Sys.time()

caret_ridge <- train(blueprint,
                     data      = teacher_tr,
                     method    = "glmnet",
                     family    = "binomial",
                     metric    = "logLoss",
                     trControl = cv,
                     tuneGrid = grid_ridge)

Sys.time()

```

```{r}
plot(caret_ridge)

# Optimal lambda ridge penalty
LL_ridge <- caret_ridge$bestTune$lambda

```

### 3.2 Logistic Regression Performance Evaluation

```{r}
# Performance Evaluation
predicted_ridge_te <- predict(caret_ridge, teacher_te, type = 'prob')
dim(predicted_ridge_te)
head(predicted_ridge_te)

```

```{r, fig.width = 8, fig.height = 6}
# Check the separation of distribution for the 
# predicted probabilities between the two classes

plot(density(predicted_ridge_te[group_0,]$Never, adjust = 1.5), xlab = '', main = "Logistic Model_ridge penalty")
points(density(predicted_ridge_te[group_1,]$Never, adjust = 1.5),lty=2,type='l')

legend(x=-0.2, y=1.5, c("AnyUse","Never"),lty=c(1,2), bty='n')
```



```{r}
cut.obj_ridge <- cutpointr(x = predicted_ridge_te$Never,
                           class = teacher_te$assign_pro_bi)

AUC_ridge <- auc(cut.obj_ridge)
```

```{r}
pred_ridge_class <- ifelse(predicted_ridge_te$Never>.5,1,0)
confusion_ridge <- table(teacher_te$assign_pro_bi, pred_ridge_class)
confusion_ridge

```

```{r}
# True Negative Rate
TNR_ridge <- confusion_ridge[1,1]/(confusion_ridge[1,1]+confusion_ridge[1,2])

# False Positive Rate
FPR_ridge <- confusion_ridge[1,2]/(confusion_ridge[1,1]+confusion_ridge[1,2])

# True Positive Rate
TPR_ridge <- confusion_ridge[2,2]/(confusion_ridge[2,1]+confusion_ridge[2,2])

# Precision
PRE_ridge <- confusion_ridge[2,2]/(confusion_ridge[1,2]+confusion_ridge[2,2])

# overall acuracy
ACC_ridge <- (confusion_ridge[2,2]+confusion_ridge[1,1])/(confusion_ridge[2,2]+confusion_ridge[1,1]+confusion_ridge[1,2]+confusion_ridge[2,1])
```

Using Classification with Ridge Penalty to explore the most important 10 variables correlates with our outcome variable

```{r}
require(vip)

vip(caret_ridge, num_features = 10, geom = "point") + theme_bw()
```

## 4. Building a Classification Model with Lasso Penalty

```{r}
grid_lasso <- data.frame(alpha = 1, lambda = seq(0, .001,.00001))
head(grid_lasso)
```

```{r}
Sys.time()

caret_lasso <- train(blueprint,
                     data = teacher_tr,
                     method = "glmnet",
                     family = "binomial",
                     metric = "logLoss",
                     trControl = cv,
                     tuneGrid = grid_lasso)

Sys.time()
```


```{r}
plot(caret_lasso)

LL_lasso <- caret_lasso$bestTune$lambda
```

```{r}
predicted_lasso_te <- predict(caret_lasso, teacher_te, type = "prob")
dim(predicted_lasso_te)
head(predicted_lasso_te)
```

```{r}
plot(density(predicted_lasso_te[group_0,]$Never,adjust=1.5), xlab = '', main = 'Logistic Model_lasso penalty')
points(density(predicted_lasso_te[group_1,]$Never,adjust=1.5),lty=2,type='l')
legend(x=0.8,y=0.8, c("AnyUse","Never"), lty=c(1,2), bty='n')

```

```{r}
cut.obj_lasso <- cutpointr(x = predicted_lasso_te$Never,
                           class = teacher_te$assign_pro_bi)
AUC_lasso <- auc(cut.obj_lasso)
```

```{r}
pred_lasso_class <- ifelse(predicted_lasso_te$Never>.5,1,0)
confusion_lasso <- table(teacher_te$assign_pro_bi,pred_lasso_class)
confusion_lasso
```

```{r}
# True Negative Rate
TNR_lasso <- confusion_lasso [1,1]/(confusion_lasso[1,1]+confusion_lasso[1,2])

# False Positive Rate
FPR_lasso <- confusion_lasso[1,2]/(confusion_lasso[1,1]+confusion_lasso[1,2])

# True Positive Rate
TPR_lasso <- confusion_lasso[2,2]/(confusion_lasso[2,1]+confusion_lasso[2,2])

# Precision
PRE_lasso <- confusion_lasso[2,2]/(confusion_lasso[1,2]+confusion_lasso[2,2])

# overall accuracy
ACC_lasso <- (confusion_lasso[2,2]+confusion_lasso[1,1])/(confusion_lasso[2,2]+confusion_lasso[1,1]+confusion_lasso[1,2]+confusion_lasso[2,1])
```

```{r}
vip(caret_lasso, num_features = 10, geom = "point") + theme_bw()+labs(title = "Important predictors highlighted by Lasso Penalty Model")
```

```{r}
coefs_lasso <- coef(caret_lasso$finalModel,
                    caret_lasso$bestTune$lambda)

ind_lasso <- order(abs(coefs_lasso[,1]),decreasing=T)

head(as.matrix(coefs_lasso[ind_lasso,]),10)
```


## Report
```{r}
results <- data.frame(
    Model    = c("Logistic Regression","Logistic Regression with Ridge Penalty","Logistic Regression with Lasso Penalty"),
    LL       = c(LL_mod,LL_ridge,LL_lasso),
    AUC      = c(AUC,AUC_ridge,AUC_lasso),
    ACC      = c(ACC,ACC_ridge,ACC_lasso),
    TPR      = c(TPR,TPR_ridge,TPR_lasso),
    TNR      = c(TNR,TNR_ridge,TNR_lasso),
    PRE      = c(PRE,PRE_ridge,PRE_lasso)
)

knitr::kable(results,digits = 3)
```

