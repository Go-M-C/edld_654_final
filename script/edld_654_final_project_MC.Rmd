---
title: "edld_654_final_project_MC"
author: "Michelle Cui"
date: "2025-11-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

-   **Research problem (10pts)**: Describe the task you want to achieve. What is the outcome of interest? What are you trying to predict? Why is it important? What are the potential benefits of having a predictive model for this outcome?

-   **Description of the data (15pts)**: Describe core features of the data, any additional features you produced from existing features and how, basic descriptive statistics about these features, and any missing data analysis you conduct. The description should be sufficiently clear that the instructor understands all the variables included in your modeling.

The main objective of this task is to explore several supervised learning models for predicting the outcome of interest. The Dataset I used is the US teacher questionnaire survey data provided in the International Computer and Information Literacy Study(ICILS) 2023.

Which teacher-level factors predict how frequently teachers have 8-th grade students write computer programs, games, or apps during classroom instruction?

The ICILS is designed to measure 8th-grade students' capabilities to use Computer Information Literacy(CIL) and Computational Thinking(CT). Along with 35 education systems, United States participated CIL assessment and CT assessment with 24 education systems. The study survey included context questionnaire for school principles & coordinators, teachers, and 8th-grade students. It is to my interest that, the study didn't include the U.S. teacher data in the international average or presented in the main ICILS 2023 international report because the response rate for sampled teachers in the United States was below 50 percent. The correlated factors could be varied. Thus, I plan to use this dataset to explore more about teachers' usage of ICT in the U.S.

-   **Description of the models (15pts)**: Apply at least three different modeling approaches to predict the outcome in the dataset. Describe any specific setting used during the model fitting (e.g., hyperparameter tuning, cross-validation). Also, discuss how you plan to evaluate model performance.

-   **Model fit (20pts)**: Provide the results of your model evaluation. Compare and contrasts results from different modeling approaches, including a discussion of model performance. Discuss your final model selection and the evidence that led you to this selection. If it is a classification problem, how did you choose a cut-off point for binary predictions? Did you consider different cut-off points?

-   **Data visualization (5pts)**: Include at least two plots (or more) to help communicate your findings. The plots may be of initial data explorations, fits of individual models, and plots displaying the performance of competing models.

-   **Discussion/Conclusion (25pts)**: Discuss and summarize what you learned. Which variables were the most important in predicting your outcome? Was this expected or surprising? Were different models close in performance, or were there significant gaps in performance from different modeling approaches? Are there practical/applied findings that could help the field of your interest based on your work? If yes, what are they?

-   **Reproducibility (10pts)**: Provide a link to the GitHub repo at the beginning of your report as a note.

```{r}
require(here)
require(tidyverse)
require(recipes)
require(ggplot2)
require(labelled)
require(stringr)
require(haven) #zap_labels function
```
## 1.1 Initial Data Preparation
```{r}
load(here("data","BTGUSAI3.Rdata"))
teacher <- BTGUSAI3
names(teacher)
```

```{r}
vars_to_remove <-grep(
  pattern = "^(TRWGT|WGT|JKZONET|JKREPT|SENWGTT|VERSION|HOUWGTT|TOTWGTT|T_)",
                       names(teacher), value = TRUE)
vars_to_remove
teacher_clean <- teacher[, !names(teacher) %in% vars_to_remove]
```

```{r}
#remove SPSS labels without change numeric and label
teacher_clean <- 
  teacher_clean %>% 
  zap_labels() %>% 
  zap_label()
```

```{r}

teacher_clean$IT3G17J_f <- factor(
  teacher_clean$IT3G17J,
  levels = c(1,2,3,4,5),
  labels = c(
    "Never",
    "≤ 1/4",
    "≤ 1/2",
    "1/2–3/4",
    "≥ 3/4"
  )
)

ggplot(teacher_clean, aes(IT3G17J_f, fill = IT3G17J_f)) +
  geom_bar() +
  geom_text(stat = "count",
            aes(label = after_stat(count)),
            vjust = -0.5,
            size = 4)+
  theme_minimal() +
  labs(title = "Distribution of Programming Assignment Frequency",
       x = "Write computer programs, games, or apps",
       y = "Count")
```

-   I collapsed the outcome variable into a binary indicator that reflecting whether the teacher ever engages students in computer programming activities. The original outcome is highly skewed toward "Never", and the categories above "Never" are sparse. It would be risky for me to use ordinal modeling. 1: Never; 2: One quarter or less (but more than never); 3: One half or less (but more than one quarter); 4: More than one-half (but less than three quarters); 5: Three quarters or more

```{r}
teacher_clean <- teacher_clean %>% 
  mutate(
    assign_pro_bi = ifelse(IT3G17J == 1,1,0)
  ) %>% 
  select(-IT3G17J,-IT3G17J_f,-IDCNTRY,-CNTRY, -IEADATE)
```

```{r}
# Initial data preparation
require(finalfit)
ff_glimpse(teacher_clean)

require(dplyr)

# identify variables
categorical_vars <- teacher_clean %>% 
  summarise(across(everything(), ~n_distinct(.x, na.rm = TRUE))) %>% 
  pivot_longer(everything(), names_to = "variable", values_to = "unique_n") %>% 
  filter(unique_n <= 10 | 
           sapply(teacher_clean, is.character)[match(variable, names(teacher_clean))]) %>% 
  pull(variable)

categorical_vars


# Move IDSCHOOL to categorical_var
teacher_clean$IDSCHOOL <- as.factor(teacher_clean$IDSCHOOL)
categorical_vars <- c(categorical_vars, "IDSCHOOL") %>% 
  unique()

continuous_vars <- setdiff(names(teacher_clean), categorical_vars)

continuous_vars

```

```{r}
# Convert all categorical variables to factors
teacher_clean <- teacher_clean %>% 
  mutate(across(all_of(categorical_vars), as.factor))
```

Missing summary table

```{r}
missing_ <- ff_glimpse(teacher_clean)$Continuous
head(arrange(missing_, desc(missing_percent)))

```

Group variables into different types

```{r}
teacher_clean$assign_pro_bi <- factor(teacher_clean$assign_pro_bi) 
teacher_clean <- teacher_clean %>% 
  mutate(across(all_of(categorical_vars), as.factor))

outcome <- c('assign_pro_bi')

id      <- c("IDTEACH")

catergorical <- setdiff(categorical_vars, outcome)

continuous <- setdiff(continuous_vars,id)

```

Build blueprint recipes to prepare data: I implemented the following steps:

-   Creating missing data variable indicators for: all_of(categorical) all_of(continuous)

-   Zero variance filter on: all_numeric()

-   Mean imputation for: all_of(continuous)

-   Mode imputation for: all_of(catergorical)

-   Polynomial expansion on: all_of(numeric)

-   Centering and scaling for: paste0(continuous, "\_poly_1") paste0(continuous, "\_poly_2")

-   Dummy variables from: all_of(catergorical)

-   Factor variables from: assign_pro_bi

```{r}

require(recipes)
options(warn=-1)

blueprint <- recipe(x     = teacher_clean,
                    vars  = c(catergorical,continuous,outcome,id),
                    roles = c(rep('predictor',204),'outome','ID')) %>% 
  
  # For all 204 predictors, create an indicator variable for missingness
  
  step_indicate_na(all_of(categorical), all_of(continuous)) %>% 
  
  # Remove the variable with zero variance, this will also remove 
  # The missingness variables
  
  step_zv(all_numeric()) %>% 
  
  # impute the missing values using mean and mode
  
  step_impute_mean(all_of(continuous)) %>% 
  step_impute_mode(all_of(catergorical)) %>% 
  
  # 2nd degree polynomial terms for continuous variables
  
  step_poly(all_of(numeric),degree = 2) %>% 
  
  # Standardize the polynomial terms of continuous variables
  
  step_normalize(paste0(continuous, '_poly_1'),
                 paste0(continuous, '_poly_2')) %>% 
  
  # One-hot encoding for all categorical variables
  
  step_dummy(all_of(catergorical), one_hot = TRUE) %>% 
  step_num2factor(assign_pro_bi,
                  transform = function(x)x + 1,
                  levels = c("Any use","Never"))

blueprint
```

**Building a Classification Model with Ridge Penalty**
1.2. Train/Test split

```{r}
require(caret)

loc <- sample(1:nrow(teacher_clean), round(nrow(teacher_clean)*0.8))

teacher_tr <- teacher_clean[loc, ]
teacher_te <- teacher_clean[-loc, ]

dim(teacher_tr)
dim(teacher_te)
```

```{r}
# Randomly shuffle the training dataset
set.seed(11252025)
teacher_tr = teacher_tr[sample(nrow(teacher_tr)), ]

# Create 10 folds with equal size
folds = cut(seq(1,nrow(teacher_tr)), breaks = 10, labels = FALSE)

# Create the list for each fold
my.indices <- vector('list', 10)

for (i in 1:10){
  my.indices[[i]] <- which(folds != i)
}
```

```{r}

```

